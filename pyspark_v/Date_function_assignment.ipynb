{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1272e920-e4aa-4079-b129-8c0f585aaa46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert a String Column to Date Format\n",
    "# Problem: Convert the \"DOB\" (Date of Birth) column from string format to yyyy-MM-dd.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "data= [('Anjali','12-02-1995'),('Ramesh','15-07-1998'),('Priya','09-11-2001')]\n",
    "schema =StructType([StructField('Name',StringType())\n",
    "                    ,StructField('DOB',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).withColumn(\"DOB\",to_date(col(\"DOB\"),\"dd-MM-yyyy\"))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f7ecc6-e075-491c-a943-ea8ca05ee8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Age from Date of Birth\n",
    "# Problem: Create a new column \"Age\" by calculating the age from the \"DOB\" column.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date, trim, datediff, lit,round\n",
    "\n",
    "data= [('Anjali','1993-06-25'),('Ramesh','2000-03-10'),('Priya','1996-12-15')]\n",
    "schema =StructType([StructField('Name',StringType())\n",
    "                    ,StructField('DOB',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).withColumn(\"DOB\",to_date(col(\"DOB\"),\"yyyy-MM-dd\"))\\\n",
    "                                    .withColumn(\"Age\",round( datediff(lit(\"2022-01-01\"), col(\"DOB\")) / 365, 0))\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8404a9b6-b33e-4e63-8ce1-86ed8241ad1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter Records for People Born After 2000\n",
    "# Problem: Filter out people born after January 1, 2000.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data= [('Suresh','2002-04-10'),('Sunita','1998-08-30'),('Priya','2003-01-25')]\n",
    "schema=StructType([StructField('Name',StringType())\n",
    "                    ,StructField('DOB',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "\n",
    "f=df.filter(col(\"DOB\") > \"2000-01-01\")\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b45dd4-8f05-46f3-80f2-18a2f85152c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the Month and Year of a Date\n",
    "# Problem: Extract the month and year from the \"Joining Date\" column.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType ,DateType\n",
    "from pyspark.sql.functions import col, month, year, to_date, date_format\n",
    "\n",
    "data = [('Mahesh','2020-05-12'),('Neha','2019-08-09'),('Ajay','2021-03-21')]\n",
    "schema = StructType([StructField('Name', StringType()),StructField('Joining Date', StringType())])\n",
    "\n",
    "df = (spark.createDataFrame(data, schema)\n",
    "        .withColumn('Joining Date', to_date(col('Joining Date'), 'yyyy-MM-dd')))\n",
    "\n",
    "j_date = (df.select(col('Joining Date'))\n",
    "            .withColumn('Month', date_format(col('Joining Date'), \"MMMM\")) \n",
    "            .withColumn('Year', year(col('Joining Date'))))\n",
    "\n",
    "df_1=df.join(j_date, df['Joining Date'] == j_date['Joining Date']).drop(j_date['Joining Date'])\n",
    "\n",
    "display(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde360d0-a2d7-4a75-af04-efd51496bf9c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764687366762}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the Difference Between Two Dates\n",
    "# Problem: Calculate the number of days between \"Start Date\" and \"End Date\".\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date, date_format,datediff\n",
    "\n",
    "data= [('Mahesh','2023-01-01','2023-02-01'),('Neha','2022-07-10','2022-08-01'),('Ajay','2023-03-15','2023-03-20')]\n",
    "schema=StructType([StructField('Name',StringType())\n",
    "                    ,StructField('Start Date',StringType())\n",
    "                    ,StructField('End Date',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).withColumn(\"End Date\",to_date(col(\"End Date\"),\"yyyy-MM-dd\"))\\\n",
    "            .withColumn(\"Start Date\",to_date(col(\"Start Date\"),\"yyyy-MM-dd\"))\\\n",
    "            .withColumn(\"date_diff_days\",datediff(col(\"End Date\"),col(\"Start Date\")))\n",
    "\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5034f1da-0e8c-4105-9424-84f1da74c87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add Days to a Date\n",
    "# Problem: Add 30 days to the \"Booking Date\" column.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date,date_add\n",
    "\n",
    "data= [('Mahesh','2023-06-10'),('Neha','2023-07-01'),('Ajay','2023-05-25')]\n",
    "schema=StructType([StructField('Name',StringType())\n",
    "                    ,StructField('Booking Date',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).withColumn(\"Booking Date\",to_date(col(\"Booking Date\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "date_add=df.withColumn(\"After adding date\",date_add(col(\"Booking Date\"),30))\n",
    "\n",
    "display(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0054330b-f39f-4629-87c9-062ac7350c2b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764688301360}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the Day of the Week for a Date\n",
    "# Problem: Find the day of the week for the \"DOB\" column.\n",
    "\n",
    "data= [('Mahesh','2023-06-10'),('Neha','2023-07-01'),('Ajay','2023-05-25')]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "data= [('Mahesh','2023-06-10'),('Neha','2023-07-01'),('Ajay','2023-05-25')]\n",
    "schema=StructType([StructField('Name',StringType())\n",
    "                    ,StructField('DOB',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).withColumn(\"DOB\",to_date(col(\"DOB\"),\"yyyy-MM-dd\"))\\\n",
    "                            .withColumn(\"Day of the week\",dayofweek(col(\"DOB\")))\\\n",
    "                             .withColumn(\"Day\",date_format(col(\"DOB\"),\"EEEE\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ec927a-cbb9-4d05-825c-c556ff9f1871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group Records by Month\n",
    "# Problem: Group records by the month of the \"Order Date\" and count the number of orders.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date, date_format\n",
    "\n",
    "data = [('Mahesh', '2023-06-10'),('Neha', '2023-07-01'),('Ajay', '2023-07-25')]\n",
    "\n",
    "schema = StructType([StructField('Name', StringType()),StructField('Order Date', StringType())])\n",
    "\n",
    "df = spark.createDataFrame( data,schema).withColumn(\"Order Date\",to_date(col(\"Order Date\"), \"yyyy-MM-dd\"))\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07562527-a0f2-4043-98f7-139cc8d97cad",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764691882623}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find Records for the Current Year\n",
    "# Filter out records where the \"Purchase Date\" is in the current year (2025).\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "data= [('Mahesh','2023-06-10'),('Neha','2025-01-01'),('Ajay','2025-02-02')]\n",
    "schema=StructType([StructField('Name',StringType())\n",
    "                    ,StructField('Purchase Date',StringType())])\n",
    "\n",
    "df=spark.createDataFrame(data,schema).withColumn(\"Purchase Date\",to_date(col(\"Purchase Date\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "f=df.filter(col(\"Purchase Date\") > \"2025-01-01\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d9b262-afa0-416b-9f50-75c4798aff11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify Leap Year Dates\n",
    "# Problem: Identify if the \"Event Date\" is in a leap year.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, to_date,year\n",
    "\n",
    "data = [(\"2020-02-29\",), (\"2021-06-15\",), (\"2024-03-10\",), (\"2023-07-20\",)]\n",
    "schema=StructType([StructField('Event Date', StringType())])\n",
    "\n",
    "df = spark.createDataFrame(data,schema).withColumn(\"Event Date\",to_date(col(\"Event Date\"),\"yyyy-MM-dd\"))\n",
    "\n",
    "leap=df.filter(year(col(\"Event Date\")) %4==0).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9bd0e60-a891-4c57-81ce-2e7625372f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Date_function_assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
